{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f8c4539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk # Natural Language Toolkit\n",
    "import re # Regular Expression\n",
    "import string\n",
    "import math\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n",
    "\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a585dc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessing():\n",
    "    def PreProcess(dokumen):\n",
    "        case = PreProcessing.Case_Folding(dokumen)\n",
    "        english = PreProcessing.remove_english_words(case)\n",
    "        token = PreProcessing.Tokenizing(english)\n",
    "        stopword = PreProcessing.Stopword(english)\n",
    "        stemming = PreProcessing.Stemming(stopword)\n",
    "        return stemming\n",
    "    \n",
    "    def PreProcess2(dokumen):\n",
    "        case = PreProcessing.Case_Folding(dokumen)\n",
    "        english = PreProcessing.remove_english_words(case)\n",
    "#         token = PreProcessing.Tokenizing(case)\n",
    "        return english\n",
    "        \n",
    "    def Case_Folding(query):\n",
    "        #folder = \"Hadits Bukhari-Muslim/\"\n",
    "        #query = \"hadits tentang\"\n",
    "        #data_hadits = tampil.Tampdbil_Hadis()\n",
    "        #data_tes = open(folder + data_hadits[0][2], \"r\").read()\n",
    "        lower_case = query.lower()\n",
    "        removing_number = re.sub(r\"\\d+\", \"\", lower_case)\n",
    "        hapus_tanda_baca = removing_number.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "        hasil = hapus_tanda_baca.strip()\n",
    "        return hasil\n",
    "\n",
    "    def remove_english_words(doc):\n",
    "        word_list = doc.split()\n",
    "        english_vocab = set(words.words())\n",
    "        filtered_words = [word for word in word_list if word.lower() not in english_vocab]\n",
    "        filtered_english = ' '.join(filtered_words)\n",
    "        return filtered_english\n",
    "\n",
    "    def Tokenizing(doc):\n",
    "        tes = doc.split(' ')\n",
    "        return tes\n",
    "\n",
    "    def Stopword(doc):\n",
    "        stop_words = set(stopwords.words('indonesian'))\n",
    "        stop_words.remove('bapak')\n",
    "        #print(stop_words)\n",
    "        tokens = word_tokenize(doc)\n",
    "        filtered_tokens = [word for word in tokens if word.casefold() not in stop_words]\n",
    "        filtered_text = ' '.join(filtered_tokens)\n",
    "        return filtered_text\n",
    "\n",
    "    def Stemming(doc):\n",
    "        factory = StemmerFactory()\n",
    "        stemmer = factory.create_stemmer()\n",
    "        doc = stemmer.stem(doc)\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "843fce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_preprocess_to_df(data):\n",
    "\n",
    "    preprocess = []\n",
    "    preprocess2 = []\n",
    "    preprocess_kalimat = []\n",
    "    preprocess_kalimat2 = []\n",
    "    for i in range(len(data)):\n",
    "        preprocess.append(data['textpreprocessing'][i].split())\n",
    "        isi = open(\"Hadits Bukhari-Muslim/\"+data['file'][i], \"r\").read()\n",
    "        preprocess2.append(PreProcessing.PreProcess2(isi).split(' '))\n",
    "        isi = sent_tokenize(isi.lower())\n",
    "        hasil_prepro = []\n",
    "        hasil_prepro2 = []\n",
    "        for k in range(len(isi)):\n",
    "            hasil_prepro.append(PreProcessing.PreProcess(isi[k]))\n",
    "            hasil_prepro2.append(PreProcessing.PreProcess2(isi[k]))\n",
    "        isi = \"\"\n",
    "        preprocess_kalimat.append(hasil_prepro)\n",
    "        preprocess_kalimat2.append(hasil_prepro2)\n",
    "\n",
    "    data['preprocess'] = preprocess\n",
    "    data['preprocess2'] = preprocess2\n",
    "    data['preprocess_kalimat'] = preprocess_kalimat\n",
    "    data['preprocess_kalimat2'] = preprocess_kalimat2\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "106e48d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>judul_bab</th>\n",
       "      <th>file</th>\n",
       "      <th>textpreprocessing</th>\n",
       "      <th>preprocess</th>\n",
       "      <th>preprocess2</th>\n",
       "      <th>preprocess_kalimat</th>\n",
       "      <th>preprocess_kalimat2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Hadis Tentang Wahyu</td>\n",
       "      <td>1. Hadis Tentang Wahyu/1.txt</td>\n",
       "      <td>riwayat aisyah ibu orangorang iman bahwasanya ...</td>\n",
       "      <td>[riwayat, aisyah, ibu, orangorang, iman, bahwa...</td>\n",
       "      <td>[diriwayatkan, aisyah, ibu, orangorang, berima...</td>\n",
       "      <td>[riwayat aisyah orangorang iman permualaan wah...</td>\n",
       "      <td>[diriwayatkan aisyah ibu orangorang beriman ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Hadis Tentang Wahyu</td>\n",
       "      <td>1. Hadis Tentang Wahyu/2.txt</td>\n",
       "      <td>riwayat aisyah ibu kaum mukminin alharrits bin...</td>\n",
       "      <td>[riwayat, aisyah, ibu, kaum, mukminin, alharri...</td>\n",
       "      <td>[diriwayatkan, aisyah, ibu, kaum, mukminin, ba...</td>\n",
       "      <td>[riwayat aisyah kaum mukminin alharrits hisyam...</td>\n",
       "      <td>[diriwayatkan aisyah ibu kaum mukminin bahwa a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Hadis Tentang Akidah, Iman, dan Islam</td>\n",
       "      <td>2. Hadis Tentang Akidah, Iman, dan Islam/1.txt</td>\n",
       "      <td>riwayat abu hurairah radhiallahu anhu kata nab...</td>\n",
       "      <td>[riwayat, abu, hurairah, radhiallahu, anhu, ka...</td>\n",
       "      <td>[diriwayatkan, hurairah, radhiallahu, anhu, be...</td>\n",
       "      <td>[riwayat hurairah radhiallahu anhu nabi shalla...</td>\n",
       "      <td>[diriwayatkan hurairah radhiallahu anhu berkat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Hadis Tentang Akidah, Iman, dan Islam</td>\n",
       "      <td>2. Hadis Tentang Akidah, Iman, dan Islam/2.txt</td>\n",
       "      <td>riwayat anas bin malik nabi shallallahu alaihi...</td>\n",
       "      <td>[riwayat, anas, bin, malik, nabi, shallallahu,...</td>\n",
       "      <td>[diriwayatkan, anas, nabi, shallallahu, wassal...</td>\n",
       "      <td>[riwayat anas nabi shallallahu wassalam beliau...</td>\n",
       "      <td>[diriwayatkan anas nabi shallallahu wassalam b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Hadis Tentang Akidah, Iman, dan Islam</td>\n",
       "      <td>2. Hadis Tentang Akidah, Iman, dan Islam/3.txt</td>\n",
       "      <td>diriwiyatkan abu hurairah radhiallahu anhu kat...</td>\n",
       "      <td>[diriwiyatkan, abu, hurairah, radhiallahu, anh...</td>\n",
       "      <td>[diriwiyatkan, hurairah, radhiallahu, anhu, di...</td>\n",
       "      <td>[diriwiyatkan hurairah radhiallahu anhu rasulu...</td>\n",
       "      <td>[diriwiyatkan hurairah radhiallahu anhu dia be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>277</td>\n",
       "      <td>Hadis Tentang Hak dan Kasih Sayang</td>\n",
       "      <td>29. Hadis Tentang Hak dan Kasih Sayang/8.txt</td>\n",
       "      <td>riwayat muadz radliallahu anhu berkataaku pern...</td>\n",
       "      <td>[riwayat, muadz, radliallahu, anhu, berkataaku...</td>\n",
       "      <td>[diriwayatkan, muadz, radliallahu, anhu, ia, b...</td>\n",
       "      <td>[riwayat muadz radliallahu anhu berkataaku bon...</td>\n",
       "      <td>[diriwayatkan muadz radliallahu anhu ia berkat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>278</td>\n",
       "      <td>Hadis Tentang Hak dan Kasih Sayang</td>\n",
       "      <td>29. Hadis Tentang Hak dan Kasih Sayang/9.txt</td>\n",
       "      <td>riwayat abu hurairah radliallahu anhu kata ras...</td>\n",
       "      <td>[riwayat, abu, hurairah, radliallahu, anhu, ka...</td>\n",
       "      <td>[diriwayatkan, hurairah, radliallahu, anhu, ia...</td>\n",
       "      <td>[riwayat hurairah radliallahu anhu rasulullah ...</td>\n",
       "      <td>[diriwayatkan hurairah radliallahu anhu ia ber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>279</td>\n",
       "      <td>Hadis Tentang Hak dan Kasih Sayang</td>\n",
       "      <td>29. Hadis Tentang Hak dan Kasih Sayang/10.txt</td>\n",
       "      <td>riwayat uqbah bin amr kata hudzaifahtidakkah k...</td>\n",
       "      <td>[riwayat, uqbah, bin, amr, kata, hudzaifahtida...</td>\n",
       "      <td>[diriwayatkan, uqbah, amr, ia, berkata, kepada...</td>\n",
       "      <td>[riwayat uqbah amr hudzaifahtidakkah sedia cer...</td>\n",
       "      <td>[diriwayatkan uqbah amr ia berkata kepada hudz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>280</td>\n",
       "      <td>Hadis Tentang Ujian dan Kesabaran</td>\n",
       "      <td>30. Hadis Tentang Ujian dan Kesabaran/1.txt</td>\n",
       "      <td>riwayat aisyah radiallahuanha istri nabi shall...</td>\n",
       "      <td>[riwayat, aisyah, radiallahuanha, istri, nabi,...</td>\n",
       "      <td>[diriwayatkan, aisyah, radiallahuanha, istri, ...</td>\n",
       "      <td>[riwayat aisyah radiallahuanha istri nabi shal...</td>\n",
       "      <td>[diriwayatkan aisyah radiallahuanha istri nabi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>281</td>\n",
       "      <td>Hadis Tentang Masjid</td>\n",
       "      <td>31. Hadis Tentang Masjid/1.txt</td>\n",
       "      <td>riwayat abu dzarr radiallahuanhu berkataaku ta...</td>\n",
       "      <td>[riwayat, abu, dzarr, radiallahuanhu, berkataa...</td>\n",
       "      <td>[diriwayatkan, dzarr, radiallahuanhu, ia, berk...</td>\n",
       "      <td>[riwayat dzarr radiallahuanhu berkataaku rasul...</td>\n",
       "      <td>[diriwayatkan dzarr radiallahuanhu ia berkataa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>281 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                              judul_bab  \\\n",
       "0      1                    Hadis Tentang Wahyu   \n",
       "1      2                    Hadis Tentang Wahyu   \n",
       "2      3  Hadis Tentang Akidah, Iman, dan Islam   \n",
       "3      4  Hadis Tentang Akidah, Iman, dan Islam   \n",
       "4      5  Hadis Tentang Akidah, Iman, dan Islam   \n",
       "..   ...                                    ...   \n",
       "276  277     Hadis Tentang Hak dan Kasih Sayang   \n",
       "277  278     Hadis Tentang Hak dan Kasih Sayang   \n",
       "278  279     Hadis Tentang Hak dan Kasih Sayang   \n",
       "279  280      Hadis Tentang Ujian dan Kesabaran   \n",
       "280  281                   Hadis Tentang Masjid   \n",
       "\n",
       "                                               file  \\\n",
       "0                      1. Hadis Tentang Wahyu/1.txt   \n",
       "1                      1. Hadis Tentang Wahyu/2.txt   \n",
       "2    2. Hadis Tentang Akidah, Iman, dan Islam/1.txt   \n",
       "3    2. Hadis Tentang Akidah, Iman, dan Islam/2.txt   \n",
       "4    2. Hadis Tentang Akidah, Iman, dan Islam/3.txt   \n",
       "..                                              ...   \n",
       "276    29. Hadis Tentang Hak dan Kasih Sayang/8.txt   \n",
       "277    29. Hadis Tentang Hak dan Kasih Sayang/9.txt   \n",
       "278   29. Hadis Tentang Hak dan Kasih Sayang/10.txt   \n",
       "279     30. Hadis Tentang Ujian dan Kesabaran/1.txt   \n",
       "280                  31. Hadis Tentang Masjid/1.txt   \n",
       "\n",
       "                                     textpreprocessing  \\\n",
       "0    riwayat aisyah ibu orangorang iman bahwasanya ...   \n",
       "1    riwayat aisyah ibu kaum mukminin alharrits bin...   \n",
       "2    riwayat abu hurairah radhiallahu anhu kata nab...   \n",
       "3    riwayat anas bin malik nabi shallallahu alaihi...   \n",
       "4    diriwiyatkan abu hurairah radhiallahu anhu kat...   \n",
       "..                                                 ...   \n",
       "276  riwayat muadz radliallahu anhu berkataaku pern...   \n",
       "277  riwayat abu hurairah radliallahu anhu kata ras...   \n",
       "278  riwayat uqbah bin amr kata hudzaifahtidakkah k...   \n",
       "279  riwayat aisyah radiallahuanha istri nabi shall...   \n",
       "280  riwayat abu dzarr radiallahuanhu berkataaku ta...   \n",
       "\n",
       "                                            preprocess  \\\n",
       "0    [riwayat, aisyah, ibu, orangorang, iman, bahwa...   \n",
       "1    [riwayat, aisyah, ibu, kaum, mukminin, alharri...   \n",
       "2    [riwayat, abu, hurairah, radhiallahu, anhu, ka...   \n",
       "3    [riwayat, anas, bin, malik, nabi, shallallahu,...   \n",
       "4    [diriwiyatkan, abu, hurairah, radhiallahu, anh...   \n",
       "..                                                 ...   \n",
       "276  [riwayat, muadz, radliallahu, anhu, berkataaku...   \n",
       "277  [riwayat, abu, hurairah, radliallahu, anhu, ka...   \n",
       "278  [riwayat, uqbah, bin, amr, kata, hudzaifahtida...   \n",
       "279  [riwayat, aisyah, radiallahuanha, istri, nabi,...   \n",
       "280  [riwayat, abu, dzarr, radiallahuanhu, berkataa...   \n",
       "\n",
       "                                           preprocess2  \\\n",
       "0    [diriwayatkan, aisyah, ibu, orangorang, berima...   \n",
       "1    [diriwayatkan, aisyah, ibu, kaum, mukminin, ba...   \n",
       "2    [diriwayatkan, hurairah, radhiallahu, anhu, be...   \n",
       "3    [diriwayatkan, anas, nabi, shallallahu, wassal...   \n",
       "4    [diriwiyatkan, hurairah, radhiallahu, anhu, di...   \n",
       "..                                                 ...   \n",
       "276  [diriwayatkan, muadz, radliallahu, anhu, ia, b...   \n",
       "277  [diriwayatkan, hurairah, radliallahu, anhu, ia...   \n",
       "278  [diriwayatkan, uqbah, amr, ia, berkata, kepada...   \n",
       "279  [diriwayatkan, aisyah, radiallahuanha, istri, ...   \n",
       "280  [diriwayatkan, dzarr, radiallahuanhu, ia, berk...   \n",
       "\n",
       "                                    preprocess_kalimat  \\\n",
       "0    [riwayat aisyah orangorang iman permualaan wah...   \n",
       "1    [riwayat aisyah kaum mukminin alharrits hisyam...   \n",
       "2    [riwayat hurairah radhiallahu anhu nabi shalla...   \n",
       "3    [riwayat anas nabi shallallahu wassalam beliau...   \n",
       "4    [diriwiyatkan hurairah radhiallahu anhu rasulu...   \n",
       "..                                                 ...   \n",
       "276  [riwayat muadz radliallahu anhu berkataaku bon...   \n",
       "277  [riwayat hurairah radliallahu anhu rasulullah ...   \n",
       "278  [riwayat uqbah amr hudzaifahtidakkah sedia cer...   \n",
       "279  [riwayat aisyah radiallahuanha istri nabi shal...   \n",
       "280  [riwayat dzarr radiallahuanhu berkataaku rasul...   \n",
       "\n",
       "                                   preprocess_kalimat2  \n",
       "0    [diriwayatkan aisyah ibu orangorang beriman ba...  \n",
       "1    [diriwayatkan aisyah ibu kaum mukminin bahwa a...  \n",
       "2    [diriwayatkan hurairah radhiallahu anhu berkat...  \n",
       "3    [diriwayatkan anas nabi shallallahu wassalam b...  \n",
       "4    [diriwiyatkan hurairah radhiallahu anhu dia be...  \n",
       "..                                                 ...  \n",
       "276  [diriwayatkan muadz radliallahu anhu ia berkat...  \n",
       "277  [diriwayatkan hurairah radliallahu anhu ia ber...  \n",
       "278  [diriwayatkan uqbah amr ia berkata kepada hudz...  \n",
       "279  [diriwayatkan aisyah radiallahuanha istri nabi...  \n",
       "280  [diriwayatkan dzarr radiallahuanhu ia berkataa...  \n",
       "\n",
       "[281 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"tb_hadits.csv\")\n",
    "add_preprocess_to_df(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b58c2e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class vsm():\n",
    "    def process_vsm(data, query):\n",
    "        document_term = vsm.term_document(data)\n",
    "        idf_dokumen = vsm.idf_document(data)\n",
    "        tf_idf_dokumen = vsm.tf_idf(document_term, idf_dokumen)\n",
    "        dc_vectors = vsm.document_vector(tf_idf_dokumen)\n",
    "        tfidf_query = vsm.tf_idf_query(idf_dokumen, query)\n",
    "        query_vectors = vsm.q_vector(tfidf_query)\n",
    "        scores = vsm.cosine_similarity(data, tfidf_query, tf_idf_dokumen, query_vectors, dc_vectors)\n",
    "        \n",
    "        return scores\n",
    "        \n",
    "    def term_document(data):\n",
    "        document_term = []\n",
    "        for document in data['preprocess']:\n",
    "            #Calculating the term frequency \n",
    "            term_frequency = {}\n",
    "            for term in document:\n",
    "                if term in term_frequency:\n",
    "                    term_frequency[term] += 1\n",
    "                else:\n",
    "                    term_frequency[term] = 1\n",
    "\n",
    "            document_term.append(term_frequency)\n",
    "\n",
    "        return document_term\n",
    "    \n",
    "    def idf_document(data):\n",
    "        DF = {}\n",
    "        for i in range(len(data['preprocess'])):\n",
    "            tokens = data['preprocess'][i]\n",
    "            for w in tokens:\n",
    "                try:\n",
    "                    # add token as key and doc number as value is chained\n",
    "                    DF[w].add(i)\n",
    "                except:\n",
    "                    # to handle when a new token is encountered\n",
    "                    DF[w] = {i}\n",
    "\n",
    "        for i in DF:\n",
    "            # convert to number of occurences of the token from list of documents where token occurs\n",
    "            DF[i] = len(DF[i])\n",
    "\n",
    "        idf_dokumen = {}\n",
    "        for i in DF:\n",
    "            idf_dokumen[i] = (math.log(len(data)/DF[i]))\n",
    "\n",
    "        return idf_dokumen\n",
    "    \n",
    "    def tf_idf(document_term, idf_dokumen):\n",
    "        tf_idf_dokumen = []\n",
    "        for document in document_term:\n",
    "            tf_idf = {}\n",
    "            for term in document:\n",
    "                if term in idf_dokumen:\n",
    "                    tf_idf[term] = document[term]*idf_dokumen[term]\n",
    "                    #print(document[term])\n",
    "\n",
    "            tf_idf_dokumen.append(tf_idf)\n",
    "\n",
    "        return tf_idf_dokumen\n",
    "    \n",
    "    def document_vector(tf_idf_dokumen):\n",
    "        dc_vectors = []\n",
    "        for document in tf_idf_dokumen:\n",
    "            jumlah = 0\n",
    "            for term in document.values():\n",
    "                #print(term)\n",
    "                jumlah = jumlah + (term*term)\n",
    "\n",
    "            dc_vectors.append(math.sqrt(jumlah))\n",
    "\n",
    "        return dc_vectors\n",
    "    \n",
    "    def tf_idf_query(idf_dokumen, query: str) -> list:\n",
    "        '''\n",
    "        preprocess the query\n",
    "        :param query: the query\n",
    "        '''\n",
    "        query_tf = {}\n",
    "        for term in query:\n",
    "            if term in query_tf:\n",
    "                query_tf[term] += 1\n",
    "            else:\n",
    "                query_tf[term] = 1\n",
    "\n",
    "        q_tf_idf = {}\n",
    "        for term in query_tf:\n",
    "            if term in idf_dokumen:\n",
    "                #print(term)\n",
    "                q_tf_idf[term] = query_tf[term]*idf_dokumen[term]\n",
    "\n",
    "        return q_tf_idf\n",
    "    \n",
    "    def q_vector(tfidf_query):\n",
    "        query_vector = sum([freq**2 for freq in tfidf_query.values()])\n",
    "        query_vector = math.sqrt(query_vector)\n",
    "\n",
    "        return query_vector\n",
    "    \n",
    "    def cosine_similarity(data, tfidf_query, tf_idf_dokumen, query_vector, dc_vectors):\n",
    "        scores = []\n",
    "        for i in range(len(data)):\n",
    "            tfidf = 0\n",
    "            for term in tfidf_query:\n",
    "                if term in tf_idf_dokumen[i]:\\\n",
    "                    tfidf = tfidf + (tfidf_query[term]*tf_idf_dokumen[i][term])\n",
    "\n",
    "            scores.append(tfidf/(query_vector*dc_vectors[i]))\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4f01d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class vsm2():\n",
    "    def process_vsm(data, query):\n",
    "        document_term = vsm2.term_document(data)\n",
    "        idf_dokumen = vsm2.idf_document(data)\n",
    "        tf_idf_dokumen = vsm2.tf_idf(document_term, idf_dokumen)\n",
    "        dc_vectors = vsm2.document_vector(tf_idf_dokumen)\n",
    "        tfidf_query = vsm2.tf_idf_query(idf_dokumen, query)\n",
    "        query_vectors = vsm2.q_vector(tfidf_query)\n",
    "        scores = vsm2.cosine_similarity(data, tfidf_query, tf_idf_dokumen, query_vectors, dc_vectors)\n",
    "        \n",
    "        return scores\n",
    "        \n",
    "    def term_document(data):\n",
    "        document_term = []\n",
    "        for document in data['preprocess2']:\n",
    "            #Calculating the term frequency \n",
    "            term_frequency = {}\n",
    "            for term in document:\n",
    "                if term in term_frequency:\n",
    "                    term_frequency[term] += 1\n",
    "                else:\n",
    "                    term_frequency[term] = 1\n",
    "\n",
    "            document_term.append(term_frequency)\n",
    "\n",
    "        return document_term\n",
    "    \n",
    "    def idf_document(data):\n",
    "        DF = {}\n",
    "        for i in range(len(data['preprocess2'])):\n",
    "            tokens = data['preprocess2'][i]\n",
    "            for w in tokens:\n",
    "                try:\n",
    "                    # add token as key and doc number as value is chained\n",
    "                    DF[w].add(i)\n",
    "                except:\n",
    "                    # to handle when a new token is encountered\n",
    "                    DF[w] = {i}\n",
    "\n",
    "        for i in DF:\n",
    "            # convert to number of occurences of the token from list of documents where token occurs\n",
    "            DF[i] = len(DF[i])\n",
    "\n",
    "        idf_dokumen = {}\n",
    "        for i in DF:\n",
    "            idf_dokumen[i] = (math.log(len(data)/DF[i]))\n",
    "\n",
    "        return idf_dokumen\n",
    "    \n",
    "    def tf_idf(document_term, idf_dokumen):\n",
    "        tf_idf_dokumen = []\n",
    "        for document in document_term:\n",
    "            tf_idf = {}\n",
    "            for term in document:\n",
    "                if term in idf_dokumen:\n",
    "                    tf_idf[term] = document[term]*idf_dokumen[term]\n",
    "                    #print(document[term])\n",
    "\n",
    "            tf_idf_dokumen.append(tf_idf)\n",
    "\n",
    "        return tf_idf_dokumen\n",
    "    \n",
    "    def document_vector(tf_idf_dokumen):\n",
    "        dc_vectors = []\n",
    "        for document in tf_idf_dokumen:\n",
    "            jumlah = 0\n",
    "            for term in document.values():\n",
    "                #print(term)\n",
    "                jumlah = jumlah + (term*term)\n",
    "\n",
    "            dc_vectors.append(math.sqrt(jumlah))\n",
    "\n",
    "        return dc_vectors\n",
    "    \n",
    "    def tf_idf_query(idf_dokumen, query: str) -> list:\n",
    "        '''\n",
    "        preprocess the query\n",
    "        :param query: the query\n",
    "        '''\n",
    "        query_tf = {}\n",
    "        for term in query:\n",
    "            if term in query_tf:\n",
    "                query_tf[term] += 1\n",
    "            else:\n",
    "                query_tf[term] = 1\n",
    "\n",
    "        q_tf_idf = {}\n",
    "        for term in query_tf:\n",
    "            if term in idf_dokumen:\n",
    "                #print(term)\n",
    "                q_tf_idf[term] = query_tf[term]*idf_dokumen[term]\n",
    "\n",
    "        return q_tf_idf\n",
    "    \n",
    "    def q_vector(tfidf_query):\n",
    "        query_vector = sum([freq**2 for freq in tfidf_query.values()])\n",
    "        query_vector = math.sqrt(query_vector)\n",
    "\n",
    "        return query_vector\n",
    "    \n",
    "    def cosine_similarity(data, tfidf_query, tf_idf_dokumen, query_vector, dc_vectors):\n",
    "        scores = []\n",
    "        for i in range(len(data)):\n",
    "            tfidf = 0\n",
    "            for term in tfidf_query:\n",
    "                if term in tf_idf_dokumen[i]:\\\n",
    "                    tfidf = tfidf + (tfidf_query[term]*tf_idf_dokumen[i][term])\n",
    "\n",
    "            scores.append(tfidf/(query_vector*dc_vectors[i]))\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31cca845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:24: DeprecationWarning: invalid escape sequence \\W\n",
      "<>:24: DeprecationWarning: invalid escape sequence \\W\n",
      "C:\\Users\\Rz\\AppData\\Local\\Temp/ipykernel_19828/1562428229.py:24: DeprecationWarning: invalid escape sequence \\W\n",
      "  query_expand = re.split('\\W+', query_expand)\n"
     ]
    }
   ],
   "source": [
    "def load(filename):\n",
    "    with open(filename) as data_file:\n",
    "        data = json.load(data_file)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# load dictionary\n",
    "mydict = load('dict.json')\n",
    "\n",
    "def getSinonim(word):\n",
    "    if word in mydict.keys():\n",
    "        return mydict[word]['sinonim']\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def expand_query(query):\n",
    "    query_expand = []\n",
    "    for term in query:\n",
    "        query_expand.append(term)\n",
    "        query_expand = query_expand + getSinonim(term)\n",
    "\n",
    "    query_expand = ' '.join([str(elem) for elem in query_expand])\n",
    "    query_expand = PreProcessing.PreProcess(query_expand)\n",
    "    query_expand = re.split('\\W+', query_expand)\n",
    "    \n",
    "    return query_expand\n",
    "\n",
    "def expand_query2(query):\n",
    "    query_expand = []\n",
    "    for term in query:\n",
    "        query_expand.append(term)\n",
    "        query_expand = query_expand + getSinonim(term)\n",
    "\n",
    "#     query_expand = ' '.join([str(elem) for elem in query_expand])\n",
    "#     query_expand = PreProcessing.PreProcess(query_expand)\n",
    "#     query_expand = re.split('\\W+', query_expand)\n",
    "    \n",
    "    return query_expand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86086d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class expand_query_fp_growth():\n",
    "    def process(data, dokumen_teratas, query, min_support, min_threshold):\n",
    "        data_master = expand_query_fp_growth.add_data_master_fp_growth(data, dokumen_teratas)\n",
    "        # data_master_fp_growth = add_data_master_fp_growth(data, dokumen_teratas)\n",
    "        my_transactionencoder = TransactionEncoder()\n",
    "\n",
    "        # fit the transaction encoder using the list of transaction tuples\n",
    "        my_transactionencoder.fit(data_master)\n",
    "\n",
    "        # transform the list of transaction tuples into an array of encoded transactions\n",
    "        encoded_transactions = my_transactionencoder.transform(data_master)\n",
    "\n",
    "        # convert the array of encoded transactions into a dataframe\n",
    "        encoded_transactions_df = pd.DataFrame(encoded_transactions, columns=my_transactionencoder.columns_)\n",
    "        \n",
    "#         min_support = 0.2\n",
    "        # compute the frequent itemsets using fpgriowth from mlxtend\n",
    "        from mlxtend.frequent_patterns.fpgrowth import fpgrowth\n",
    "        frequent_itemsets = fpgrowth(encoded_transactions_df, min_support=min_support, use_colnames = True)\n",
    "        \n",
    "        rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_threshold)\n",
    "        if(rules == None):\n",
    "            message = \"Data yang digunakan tidak ditemukan rulesnya\"\n",
    "            return message\n",
    "        \n",
    "        df_rules = pd.DataFrame(rules)\n",
    "        df_rules = df_rules.sort_values(['confidence', 'lift'], ascending=False)\n",
    "        \n",
    "        expand = []\n",
    "        for term in query:\n",
    "            if expand_query_fp_growth.query_expand(term, df_rules) != None:\n",
    "                expand.append(expand_query_fp_growth.query_expand(term, df_rules))\n",
    "                \n",
    "        return expand\n",
    "    \n",
    "    def add_data_master_fp_growth(data, dokumen_teratas):\n",
    "        data_master_fp_growth = []\n",
    "        for term in dokumen_teratas:\n",
    "            for kalimat in data['preprocess_kalimat'][term]:\n",
    "        #     for kalimat in eval(data['textpreprocessing_kalimat'][term]):\n",
    "                data_master_fp_growth.append(kalimat.split())\n",
    "\n",
    "        return data_master_fp_growth\n",
    "    \n",
    "    def query_expand(term, df_rules):\n",
    "        for i in range(len(df_rules)):\n",
    "            #print(list(df_rules['antecedents'].values[i])[0])\n",
    "            #print(term)\n",
    "            if len(list(df_rules['antecedents'].values[i])) > 1:\n",
    "                for j in range(len(list(df_rules['antecedents'].values[i]))):\n",
    "                    if term == list(df_rules['antecedents'].values[i])[j]:\n",
    "                        #print(j)\n",
    "                        init = list(df_rules['consequents'].values[i])[0]\n",
    "                        return init\n",
    "            else:\n",
    "                if term == list(df_rules['antecedents'].values[i])[0]:\n",
    "                    #print(1)\n",
    "                    init = list(df_rules['consequents'].values[i])[0]\n",
    "                    return init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4205ba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class expand_query_fp_growth2():\n",
    "    def process(data, dokumen_teratas, query, min_support, min_threshold):\n",
    "        data_master = expand_query_fp_growth2.add_data_master_fp_growth(data, dokumen_teratas)\n",
    "        # data_master_fp_growth = add_data_master_fp_growth(data, dokumen_teratas)\n",
    "        my_transactionencoder = TransactionEncoder()\n",
    "\n",
    "        # fit the transaction encoder using the list of transaction tuples\n",
    "        my_transactionencoder.fit(data_master)\n",
    "\n",
    "        # transform the list of transaction tuples into an array of encoded transactions\n",
    "        encoded_transactions = my_transactionencoder.transform(data_master)\n",
    "\n",
    "        # convert the array of encoded transactions into a dataframe\n",
    "        encoded_transactions_df = pd.DataFrame(encoded_transactions, columns=my_transactionencoder.columns_)\n",
    "        \n",
    "#         min_support = 0.2\n",
    "        # compute the frequent itemsets using fpgriowth from mlxtend\n",
    "        from mlxtend.frequent_patterns.fpgrowth import fpgrowth\n",
    "        frequent_itemsets = fpgrowth(encoded_transactions_df, min_support=min_support, use_colnames = True)\n",
    "        \n",
    "        rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_threshold)\n",
    "        \n",
    "        df_rules = pd.DataFrame(rules)\n",
    "        df_rules = df_rules.sort_values(['confidence', 'lift'], ascending=False)\n",
    "        \n",
    "        expand = []\n",
    "        for term in query:\n",
    "            if expand_query_fp_growth2.query_expand(term, df_rules) != None:\n",
    "                expand.append(expand_query_fp_growth2.query_expand(term, df_rules))\n",
    "                \n",
    "        return expand\n",
    "    \n",
    "    def add_data_master_fp_growth(data, dokumen_teratas):\n",
    "        data_master_fp_growth = []\n",
    "        for term in dokumen_teratas:\n",
    "            for kalimat in data['preprocess_kalimat2'][term]:\n",
    "        #     for kalimat in eval(data['textpreprocessing_kalimat'][term]):\n",
    "                data_master_fp_growth.append(kalimat.split())\n",
    "\n",
    "        return data_master_fp_growth\n",
    "    \n",
    "    def query_expand(term, df_rules):\n",
    "        for i in range(len(df_rules)):\n",
    "            #print(list(df_rules['antecedents'].values[i])[0])\n",
    "            #print(term)\n",
    "            if len(list(df_rules['antecedents'].values[i])) > 1:\n",
    "                for j in range(len(list(df_rules['antecedents'].values[i]))):\n",
    "                    if term == list(df_rules['antecedents'].values[i])[j]:\n",
    "                        #print(j)\n",
    "                        init = list(df_rules['consequents'].values[i])[0]\n",
    "                        return init\n",
    "            else:\n",
    "                if term == list(df_rules['antecedents'].values[i])[0]:\n",
    "                    #print(1)\n",
    "                    init = list(df_rules['consequents'].values[i])[0]\n",
    "                    return init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fedd4441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.VSM\n",
      "2.Sinonim + VSM\n",
      "3.VSM + FP-Growth\n",
      "4.Sinonim + VSM + FP-Growth\n",
      "5.Exit\n",
      "Masukkan Pilihan :4\n",
      "Masukkan query:hadits tentang bapak\n",
      "Query yang Menggunakan Preprocess:  hadits bapak abah ayahanda bapanda bokap buya empek kepala keluarga papi rama ramanda arsitek kreator cipta diri\n",
      "5 Dokumen Teratas yang dijadikan data master FP-Growth\n",
      "\n",
      "1)\n",
      "Title:  17. Hadis Tentang Perintah dan Larangan/29.txt\n",
      "File:  17. Hadis Tentang Perintah dan Larangan/29.txt\n",
      "Score:  0.22397003754552214\n",
      "---------------XXX---------------\n",
      "2)\n",
      "Title:  25. Hadis Tentang Penyakit dan Obat/5.txt\n",
      "File:  25. Hadis Tentang Penyakit dan Obat/5.txt\n",
      "Score:  0.1446831669028331\n",
      "---------------XXX---------------\n",
      "3)\n",
      "Title:  17. Hadis Tentang Perintah dan Larangan/10.txt\n",
      "File:  17. Hadis Tentang Perintah dan Larangan/10.txt\n",
      "Score:  0.11810284629647426\n",
      "---------------XXX---------------\n",
      "4)\n",
      "Title:  10. Hadis Tentang Jihad/16.txt\n",
      "File:  10. Hadis Tentang Jihad/16.txt\n",
      "Score:  0.11489718901046829\n",
      "---------------XXX---------------\n",
      "5)\n",
      "Title:  10. Hadis Tentang Jihad/1.txt\n",
      "File:  10. Hadis Tentang Jihad/1.txt\n",
      "Score:  0.10935214683910259\n",
      "---------------XXX---------------\n",
      "\n",
      "\n",
      "Query setelah diExpand menggunakan FP-Growth  :  hadits bapak abah ayahanda bapanda bokap buya empek kepala keluarga papi rama ramanda arsitek kreator cipta diri\n",
      "Hasil VSM menggunakan Preprocess\n",
      "\n",
      "1)\n",
      "Title:  17. Hadis Tentang Perintah dan Larangan/29.txt\n",
      "File:  17. Hadis Tentang Perintah dan Larangan/29.txt\n",
      "Score:  0.22397003754552214\n",
      "---------------XXX---------------\n",
      "2)\n",
      "Title:  25. Hadis Tentang Penyakit dan Obat/5.txt\n",
      "File:  25. Hadis Tentang Penyakit dan Obat/5.txt\n",
      "Score:  0.1446831669028331\n",
      "---------------XXX---------------\n",
      "3)\n",
      "Title:  17. Hadis Tentang Perintah dan Larangan/10.txt\n",
      "File:  17. Hadis Tentang Perintah dan Larangan/10.txt\n",
      "Score:  0.11810284629647426\n",
      "---------------XXX---------------\n",
      "4)\n",
      "Title:  10. Hadis Tentang Jihad/16.txt\n",
      "File:  10. Hadis Tentang Jihad/16.txt\n",
      "Score:  0.11489718901046829\n",
      "---------------XXX---------------\n",
      "5)\n",
      "Title:  10. Hadis Tentang Jihad/1.txt\n",
      "File:  10. Hadis Tentang Jihad/1.txt\n",
      "Score:  0.10935214683910259\n",
      "---------------XXX---------------\n",
      "\n",
      "Query tanpa Preprocess :  hadits tentang akan atas bab berhubungan berkenaan mengenai perkara terhadap bapak aba abah abu ayah ayahanda babe bapanda bokap  buya empek kepala keluarga orang tua papa papi rama ramanda arsitek inventor kreator pencipta pendiri\n",
      "5 Dokumen Teratas yang dijadikan data master FP-Growth\n",
      "\n",
      "1)\n",
      "Title:  25. Hadis Tentang Penyakit dan Obat/5.txt\n",
      "File:  25. Hadis Tentang Penyakit dan Obat/5.txt\n",
      "Score:  0.13415056924821722\n",
      "---------------XXX---------------\n",
      "2)\n",
      "Title:  26. Hadis Tentang Keistimewaan Nabi dan Umatnya/5.txt\n",
      "File:  26. Hadis Tentang Keistimewaan Nabi dan Umatnya/5.txt\n",
      "Score:  0.12202160567016611\n",
      "---------------XXX---------------\n",
      "3)\n",
      "Title:  18. Hadis Tentang Etika/27.txt\n",
      "File:  18. Hadis Tentang Etika/27.txt\n",
      "Score:  0.1162475933696321\n",
      "---------------XXX---------------\n",
      "4)\n",
      "Title:  13. Hadis Tentang Alam Kubur dan Hari Akhir/13.txt\n",
      "File:  13. Hadis Tentang Alam Kubur dan Hari Akhir/13.txt\n",
      "Score:  0.10351140234775277\n",
      "---------------XXX---------------\n",
      "5)\n",
      "Title:  5. Hadis Tentang Thaharah/4.txt\n",
      "File:  5. Hadis Tentang Thaharah/4.txt\n",
      "Score:  0.07838989513940124\n",
      "---------------XXX---------------\n",
      "\n",
      "\n",
      "Query setelah diExpand menggunakan FP-Growth  :  hadits tentang akan atas bab berhubungan berkenaan mengenai perkara terhadap bapak aba abah abu ayah ayahanda babe bapanda bokap  buya empek kepala keluarga orang tua papa papi rama ramanda arsitek inventor kreator pencipta pendiri\n",
      "Hasil VSM tanpa Preprocess\n",
      "\n",
      "1)\n",
      "Title:  10. Hadis Tentang Jihad/16.txt\n",
      "File:  10. Hadis Tentang Jihad/16.txt\n",
      "Score:  0.11489718901046829\n",
      "---------------XXX---------------\n",
      "2)\n",
      "Title:  25. Hadis Tentang Penyakit dan Obat/5.txt\n",
      "File:  25. Hadis Tentang Penyakit dan Obat/5.txt\n",
      "Score:  0.1446831669028331\n",
      "---------------XXX---------------\n",
      "3)\n",
      "Title:  26. Hadis Tentang Keistimewaan Nabi dan Umatnya/5.txt\n",
      "File:  26. Hadis Tentang Keistimewaan Nabi dan Umatnya/5.txt\n",
      "Score:  0.07163476399724633\n",
      "---------------XXX---------------\n",
      "4)\n",
      "Title:  18. Hadis Tentang Etika/27.txt\n",
      "File:  18. Hadis Tentang Etika/27.txt\n",
      "Score:  0.0\n",
      "---------------XXX---------------\n",
      "5)\n",
      "Title:  13. Hadis Tentang Alam Kubur dan Hari Akhir/13.txt\n",
      "File:  13. Hadis Tentang Alam Kubur dan Hari Akhir/13.txt\n",
      "Score:  0.0\n",
      "---------------XXX---------------Apakah anda ingin mencoba menu lain ? (y/n)n\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    pilihan = input('1.VSM\\n2.Sinonim + VSM\\n3.VSM + FP-Growth\\n4.Sinonim + VSM + FP-Growth\\n5.Exit\\nMasukkan Pilihan :')\n",
    "    if pilihan == '1':\n",
    "        query = input('Masukkan query:')\n",
    "        query_prepro = PreProcessing.PreProcess(query)\n",
    "        query_prepro = re.split('\\W+', query_prepro)\n",
    "        query = PreProcessing.PreProcess2(query)\n",
    "        query = re.split('\\W+', query)\n",
    "#         print(query_prepro)\n",
    "        score_vsm = vsm.process_vsm(data, query_prepro)\n",
    "        score_vsm2 = vsm2.process_vsm(data, query)\n",
    "        print('Query Menggunakan Preprocess : ', ' '.join([str(elem) for elem in query_prepro]))\n",
    "        print('Hasil VSM Menggunakan Preprocess')\n",
    "        dokumen_teratas = sorted(range(len(score_vsm)), key=lambda i: score_vsm[i], reverse=True)[:5]\n",
    "        for i in range(5):\n",
    "            print('\\n' + str(i+1) + ')\\nTitle: ', data.iloc[dokumen_teratas[i]]['file'])\n",
    "            print('File: ', data.iloc[dokumen_teratas[i]]['file'])\n",
    "            print('Score: ', score_vsm[dokumen_teratas[i]], end=\"\\n---------------XXX---------------\")\n",
    "        \n",
    "        print('\\n')\n",
    "        print('Query tanpa Preprocess: ', ' '.join([str(elem) for elem in query]))\n",
    "        print('Hasil VSM tanpa Preprocess')\n",
    "        dokumen_teratas2 = sorted(range(len(score_vsm2)), key=lambda i: score_vsm2[i], reverse=True)[:5]\n",
    "        for i in range(5):\n",
    "            print('\\n' + str(i+1) + ')\\nTitle: ', data.iloc[dokumen_teratas2[i]]['file'])\n",
    "            print('File: ', data.iloc[dokumen_teratas2[i]]['file'])\n",
    "            print('Score: ', score_vsm2[dokumen_teratas2[i]], end=\"\\n---------------XXX---------------\")\n",
    "            \n",
    "        lanjut = input(\"Apakah anda ingin mencoba menu lain ? (y/n)\")\n",
    "        if lanjut == 'y':\n",
    "            clear_output(wait=True)\n",
    "        else:\n",
    "            break\n",
    "    elif pilihan == '2':\n",
    "        query = input('Masukkan query:')\n",
    "        query_prepro = PreProcessing.PreProcess(query)\n",
    "        query_prepro = re.split('\\W+', query_prepro)\n",
    "        query_prepro = expand_query(query_prepro)\n",
    "        score_vsm = vsm.process_vsm(data, query_prepro)\n",
    "        print('Query Menggunakan Preprocess Setelah diExpand: ', ' '.join([str(elem) for elem in query_prepro]))\n",
    "        print('Hasil VSM Menggunakan Preprocess')\n",
    "        dokumen_teratas = sorted(range(len(score_vsm)), key=lambda i: score_vsm[i], reverse=True)[:5]\n",
    "        for i in range(5):\n",
    "            print('\\n' + str(i+1) + ')\\nTitle: ', data.iloc[dokumen_teratas[i]]['file'])\n",
    "            print('File: ', data.iloc[dokumen_teratas[i]]['file'])\n",
    "            print('Score: ', score_vsm[dokumen_teratas[i]], end=\"\\n---------------XXX---------------\")\n",
    "        \n",
    "        query = PreProcessing.PreProcess2(query)\n",
    "        query = re.split('\\W+', query)\n",
    "        query = expand_query2(query)\n",
    "        score_vsm2 = vsm2.process_vsm(data, query)    \n",
    "        print('\\n')\n",
    "        print('Query tanpa Preprocess Setelah diExpand: ', ' '.join([str(elem) for elem in query]))\n",
    "        print('Hasil VSM tanpa Preprocess')\n",
    "        dokumen_teratas2 = sorted(range(len(score_vsm2)), key=lambda i: score_vsm2[i], reverse=True)[:5]\n",
    "        for i in range(5):\n",
    "            print('\\n' + str(i+1) + ')\\nTitle: ', data.iloc[dokumen_teratas2[i]]['file'])\n",
    "            print('File: ', data.iloc[dokumen_teratas2[i]]['file'])\n",
    "            print('Score: ', score_vsm2[dokumen_teratas2[i]], end=\"\\n---------------XXX---------------\")\n",
    "            \n",
    "        lanjut = input(\"Apakah anda ingin mencoba menu lain ? (y/n)\")\n",
    "        if lanjut == 'y':\n",
    "            clear_output(wait=True)\n",
    "        else:\n",
    "            break\n",
    "    elif pilihan == '3':\n",
    "        query = input('Masukkan query:')\n",
    "        query_prepro = PreProcessing.PreProcess(query)\n",
    "        query_prepro = re.split('\\W+', query_prepro)\n",
    "        score_vsm = vsm.process_vsm(data, query_prepro)\n",
    "        print('Query yang Menggunakan Preprocess: ', ' '.join([str(elem) for elem in query_prepro]))\n",
    "        print('5 Dokumen Teratas yang dijadikan data master FP-Growth')            \n",
    "        dokumen_teratas = sorted(range(len(score_vsm)), key=lambda i: score_vsm[i], reverse=True)[:5]\n",
    "        for i in range(5):\n",
    "            print('\\n' + str(i+1) + ')\\nTitle: ', data.iloc[dokumen_teratas[i]]['file'])\n",
    "            print('File: ', data.iloc[dokumen_teratas[i]]['file'])\n",
    "            print('Score: ', score_vsm[dokumen_teratas[i]], end=\"\\n---------------XXX---------------\")\n",
    "\n",
    "        eq_fp_growth = expand_query_fp_growth.process(data, dokumen_teratas, query_prepro, 0.1, 0.5)\n",
    "        query_expand = query_prepro + eq_fp_growth\n",
    "        query_expand = ' '.join([str(elem) for elem in query_expand])\n",
    "        \n",
    "        print('\\n')\n",
    "        print('\\nQuery setelah diExpand menggunakan FP-Growth  : ', query_expand)\n",
    "        query_expand = re.split('\\W+', query_expand)\n",
    "        score_akhir = vsm.process_vsm(data, query_expand)\n",
    "        print('Hasil VSM menggunakan Preprocess')\n",
    "        hasil = sorted(range(len(score_akhir)), key=lambda i: score_akhir[i], reverse=True)[:5]\n",
    "        for i in range(5):\n",
    "            print('\\n' + str(i+1) + ')\\nTitle: ', data.iloc[hasil[i]]['file'])\n",
    "            print('File: ', data.iloc[hasil[i]]['file'])\n",
    "            print('Score: ', score_akhir[hasil[i]], end=\"\\n---------------XXX---------------\")\n",
    "        \n",
    "        query = PreProcessing.PreProcess2(query)\n",
    "        query = re.split('\\W+', query)\n",
    "        score_vsm2 = vsm2.process_vsm(data, query)    \n",
    "        print('\\n')\n",
    "        print('Query tanpa Preprocess : ', ' '.join([str(elem) for elem in query]))\n",
    "        print('5 Dokumen Teratas yang dijadikan data master FP-Growth')\n",
    "        dokumen_teratas2 = sorted(range(len(score_vsm2)), key=lambda i: score_vsm2[i], reverse=True)[:5]\n",
    "        for i in range(5):\n",
    "            print('\\n' + str(i+1) + ')\\nTitle: ', data.iloc[dokumen_teratas2[i]]['file'])\n",
    "            print('File: ', data.iloc[dokumen_teratas2[i]]['file'])\n",
    "            print('Score: ', score_vsm2[dokumen_teratas2[i]], end=\"\\n---------------XXX---------------\")\n",
    "        \n",
    "        eq_fp_growth2 = expand_query_fp_growth2.process(data, dokumen_teratas, query, 0.1, 0.5)\n",
    "        query_expand2 = query + eq_fp_growth2\n",
    "        query_expand2 = ' '.join([str(elem) for elem in query_expand2])\n",
    "        print('\\n')\n",
    "        print('\\nQuery setelah diExpand menggunakan FP-Growth  : ', query_expand2)\n",
    "        query_expand2 = re.split('\\W+', query_expand2)\n",
    "        score_akhir2 = vsm2.process_vsm(data, query_expand2)\n",
    "        print('Hasil VSM tanpa Preprocess')\n",
    "        hasil2 = sorted(range(len(score_akhir2)), key=lambda i: score_akhir2[i], reverse=True)[:5]\n",
    "        for i in range(5):\n",
    "            print('\\n' + str(i+1) + ')\\nTitle: ', data.iloc[hasil2[i]]['file'])\n",
    "            print('File: ', data.iloc[hasil2[i]]['file'])\n",
    "            print('Score: ', score_akhir[hasil2[i]], end=\"\\n---------------XXX---------------\")\n",
    "            \n",
    "        lanjut = input(\"Apakah anda ingin mencoba menu lain ? (y/n)\")\n",
    "        if lanjut == 'y':\n",
    "            clear_output(wait=True)\n",
    "        else:\n",
    "            break\n",
    "    elif pilihan == '4':\n",
    "        query = input('Masukkan query:')\n",
    "        query_prepro = PreProcessing.PreProcess(query)\n",
    "        query_prepro = re.split('\\W+', query_prepro)\n",
    "        query_prepro = expand_query(query_prepro)\n",
    "        score_vsm = vsm.process_vsm(data, query_prepro)\n",
    "        print('Query yang Menggunakan Preprocess: ', ' '.join([str(elem) for elem in query_prepro]))\n",
    "        print('5 Dokumen Teratas yang dijadikan data master FP-Growth')            \n",
    "        dokumen_teratas = sorted(range(len(score_vsm)), key=lambda i: score_vsm[i], reverse=True)[:5]\n",
    "        for i in range(5):\n",
    "            print('\\n' + str(i+1) + ')\\nTitle: ', data.iloc[dokumen_teratas[i]]['file'])\n",
    "            print('File: ', data.iloc[dokumen_teratas[i]]['file'])\n",
    "            print('Score: ', score_vsm[dokumen_teratas[i]], end=\"\\n---------------XXX---------------\")\n",
    "\n",
    "        eq_fp_growth = expand_query_fp_growth.process(data, dokumen_teratas, query_prepro, 0.1, 0.5)\n",
    "        query_expand = query_prepro + eq_fp_growth\n",
    "        query_expand = ' '.join([str(elem) for elem in query_expand])\n",
    "        \n",
    "        print('\\n')\n",
    "        print('\\nQuery setelah diExpand menggunakan FP-Growth  : ', query_expand)\n",
    "        query_expand = re.split('\\W+', query_expand)\n",
    "        score_akhir = vsm.process_vsm(data, query_expand)\n",
    "        print('Hasil VSM menggunakan Preprocess')\n",
    "        hasil = sorted(range(len(score_akhir)), key=lambda i: score_akhir[i], reverse=True)[:5]\n",
    "        for i in range(5):\n",
    "            print('\\n' + str(i+1) + ')\\nTitle: ', data.iloc[hasil[i]]['file'])\n",
    "            print('File: ', data.iloc[hasil[i]]['file'])\n",
    "            print('Score: ', score_akhir[hasil[i]], end=\"\\n---------------XXX---------------\")\n",
    "        \n",
    "        query = PreProcessing.PreProcess2(query)\n",
    "        query = re.split('\\W+', query)\n",
    "        query = expand_query2(query)\n",
    "        score_vsm2 = vsm2.process_vsm(data, query)    \n",
    "        print('\\n')\n",
    "        print('Query tanpa Preprocess : ', ' '.join([str(elem) for elem in query]))\n",
    "        print('5 Dokumen Teratas yang dijadikan data master FP-Growth')\n",
    "        dokumen_teratas2 = sorted(range(len(score_vsm2)), key=lambda i: score_vsm2[i], reverse=True)[:5]\n",
    "        for i in range(5):\n",
    "            print('\\n' + str(i+1) + ')\\nTitle: ', data.iloc[dokumen_teratas2[i]]['file'])\n",
    "            print('File: ', data.iloc[dokumen_teratas2[i]]['file'])\n",
    "            print('Score: ', score_vsm2[dokumen_teratas2[i]], end=\"\\n---------------XXX---------------\")\n",
    "        \n",
    "        eq_fp_growth2 = expand_query_fp_growth2.process(data, dokumen_teratas, query, 0.1, 0.5)\n",
    "        query_expand2 = query + eq_fp_growth2\n",
    "        query_expand2 = ' '.join([str(elem) for elem in query_expand2])\n",
    "        print('\\n')\n",
    "        print('\\nQuery setelah diExpand menggunakan FP-Growth  : ', query_expand2)\n",
    "        query_expand2 = re.split('\\W+', query_expand2)\n",
    "        score_akhir2 = vsm2.process_vsm(data, query_expand2)\n",
    "        print('Hasil VSM tanpa Preprocess')\n",
    "        hasil2 = sorted(range(len(score_akhir2)), key=lambda i: score_akhir2[i], reverse=True)[:5]\n",
    "        for i in range(5):\n",
    "            print('\\n' + str(i+1) + ')\\nTitle: ', data.iloc[hasil2[i]]['file'])\n",
    "            print('File: ', data.iloc[hasil2[i]]['file'])\n",
    "            print('Score: ', score_akhir[hasil2[i]], end=\"\\n---------------XXX---------------\")\n",
    "            \n",
    "        lanjut = input(\"Apakah anda ingin mencoba menu lain ? (y/n)\")\n",
    "        if lanjut == 'y':\n",
    "            clear_output(wait=True)\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
